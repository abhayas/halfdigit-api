01: Passenger Survival Engine
Scikit-learn
Flask
REST API
● LIVE
Production-grade classification system. Features serverless cold-start handling, request logging, and real-time probability inference.

Initialize Module
Notebook Specs
↗
02: Audio Extraction Pipeline
OpenAI Whisper
Hugging Face
Flask
● LIVE
Automated speech-to-text transcription pipeline using the Whisper Large-v3 model via Hugging Face Inference API. Handles WAV/MP3 audio ingestion.

Initialize Module
Frontend Code
↗
Backend API
↗
03: Deep Learning Risk Assessor
TensorFlow/Keras
Python
Microservice
◌ BUILDING
Neural Network for financial risk assessment. Currently optimizing model weights for containerized deployment on Render free tier.

Coming Soon...
Model Specs
↗
04: Enterprise Doc Chat (RAG)
OpenAI
Vector DB
LangChain
◌ BUILDING
Retrieval-Augmented Generation system allowing secure Q&A against uploaded PDF documentation. Simulating enterprise search.

Coming Soon...
No Specs

Module 01: Titanic Survival Engine
Status: Active | Endpoint: /predict-titanic

API CONNECTION: SECURE
About this demo
This interactive demo shows a machine learning model that predicts whether a passenger on the RMS Titanic would have survived based on a few simple details. It is an educational tool — not a perfect prediction — that helps illustrate how models use features like age, sex, and travel class to estimate outcomes.

Inputs: Passenger class (1 = highest, 3 = lowest), age, sex, family count, and simple flags.
Output: A prediction ("Passenger Survived" or "Did Not Survive") plus a probability showing the model's confidence.
How it works: Your inputs are sent to a backend model which returns the prediction in real time. Change the input paraments in the below form and and click on "Run Inference Engine" to see the prediction

Module 02: Audio Extraction Pipeline
Status: Active | Endpoint: /speech-to-text

API CONNECTION: SECURE
About this demo
This module demonstrates an automated pipeline for converting unstructured audio data into structured text. It leverages the OpenAI Whisper Large-v3 model via Hugging Face Inference API to transcribe speech with high accuracy.

Inputs: Raw audio files (.wav or .mp3).
Output: Full text transcription.
Tech Stack: Next.js Frontend → Python Flask API → Hugging Face Inference Cluster.

Project Name: Portfolio AI Assistant (RAG Chatbot)
Role: AI Engineer & Full Stack Developer
Technologies: Python, Flask, LangChain, OpenAI GPT-4o, FAISS, React.js, Tailwind CSS, Hugging Face (Whisper).

Description:
Designed and developed a custom conversational AI agent embedded into the personal portfolio website (HalfDigit.com). The bot serves as an interactive resume, capable of answering specific questions about Abhaya's professional background, technical skills, and project experience in real-time.

Technical Architecture:
1. Retrieval-Augmented Generation (RAG):
   - Implemented a RAG pipeline using LangChain to ground LLM responses in factual data.
   - The knowledge base consists of structured text files (Resume, Project Details, Profile) indexed for semantic search.

2. Backend & AI Logic:
   - Built a Python Flask API to handle chat requests.
   - Utilized OpenAI's 'gpt-4o-mini' model for natural language generation and 'text-embedding-3-small' for creating high-dimensional vector embeddings.
   - Integrated FAISS (Facebook AI Similarity Search) as the vector database for efficient, low-latency document retrieval.
   - Implemented a 'Manual LCEL' (LangChain Expression Language) chain to ensure robust architecture independent of library version changes.

3. Frontend & UI:
   - Developed a responsive, floating chat widget using React.js and Tailwind CSS.
   - Features include auto-scrolling, a seamless toggle animation, and a "typing" indicator state.
   - Connected the frontend to the backend via RESTful API endpoints.

4. Audio Features:
   - Integrated Hugging Face's Inference API (Whisper model) to enable Speech-to-Text capabilities, allowing users to ask questions via voice.

Key Achievements:
- Successfully reduced hallucination by strictly constraining the AI to the retrieved context.
- Optimized cost and latency by using the 'mini' model variant and local vector storage.
- Created a "Meta-aware" system where the bot understands its own architecture and can explain how it was built.